\documentclass[11pt]{article} % set font size to 12
\usepackage[utf8]{inputenc}
\usepackage{indentfirst} % indent first paragraph
\setlength{\parskip}{1em} % line spacing between paragraphs
\setlength{\parindent}{0em} % paragraph indentation
\usepackage[margin=0.75in]{geometry} % set custom margins
\usepackage{amsmath} % for math equations
\usepackage{ amssymb } % for math symbols
\usepackage{graphicx} % for images
\usepackage{wrapfig} % for wrapped images
% Allows us to have clickable references. This should be the last package to be imported!
\usepackage{hyperref}

\title{Miscellaneous RL Notes}
\author{}
\date{}

\begin{document}
\maketitle

\tableofcontents

\newpage

\section{Introduction}
The following notes were taken from Berkeley's CS 285 course. The key notion of RL is to learn decision making and control from experience.

\subsection{Difference from supervised learning}
\begin{itemize}
    \item Supervised learning usually assumes independent, identically distributed data
    \item Have ground truth outputs in training
\end{itemize}
In RL, data is \emph{not} iid, as previous outputs influence future inputs! And we don't have ground truth data, we only know if success or failed!

\subsection{Why Deep RL?}
Deep learning allows us to handle unstructured environments. It allows us to generalize to things that we have not previously seen before. Merging deep learning with RL allows us to perform end-to-end training, such that the optimal features are automatically learnt so that we do not have to manually extract features to learn from.

Having an end-to-end training in RL meant that we do not have to separate recognition from control. For instance, we do not need a separate perception system to recognize that it is a tiger, then the control system tells us to run; rather this loop is closed with deep RL.

In short, Deep = process complex sensory input (perception), RL = can choose complex actions (control)

\subsection{Other forms of supervision}
"A cheetah would not randomly maul a gazelle and realized that it obtained a reward, therefore it should continue hunting for gazelles". There must be more detailed guiding mechanisms:\\
Learning from demonstrations.
\begin{itemize}
    \item Directly copying observed behaviour (imitation/curricular learning)
    \item Inferring rewards from observed behaviour (inverse reinforcement learning)
\end{itemize}
Learning from observing the world
\begin{itemize}
    \item Learning to predict (model-based RL)
    \item Unsupervised learning / self-supervised learning
\end{itemize}
Learning from other tasks
\begin{itemize}
    \item Transfer learning
    \item Meta-learning: learning to learn
\end{itemize}

\subsection{Building intelligent machines}
Learning is seen as the basis of intelligence - humans have to learn many tasks over their lifespan, such as driving a car

Experiments have shown that neural-networks learn similar features as a brain. Perhaps it is not because neural-networks work like a brain, but rather because those features were the right features for the data, and any model powerful enough will eventually learn those optimal features.

\subsubsection{What can deep learning and RL do well now?}
\begin{itemize}
    \item Acquire high deg. of proficiency in domains with simple, known rules - video games etc.
    \item Learn simple skills with raw sensory inputs
    \item Imitate human-provided expert behaviour
\end{itemize}

\subsubsection{What is still challenging?}
\begin{itemize}
    \item Humans can learn incredibly quickly 
    \item Humans can reuse past knowledge - transfer-learning in deep RL is an open problem
    \item Not clear what the reward function should be
    \item Not clear what the role of prediction should be - planning, or trial-and-error, or both?
\end{itemize}

\subsection{Difference between state and observation}
State is the representation of the current config of the system - its the true configuration of the state,  while the observation is what the agent actually observes. 

The state also satisfies the Markov property, which states that the future is conditionally independent from the past given the present. However, the observation might not be be sufficient to fully determine the future without observing the past - in other words, observations might not satisfy the Markov property. 

\section{Imitation Learning}
Also known as behavioural cloning, the core idea of imitation learning is to generate a set of training data and perform supervised learning on that data.

Often (but not always) insufficient by itself, due to distribution mismatch problem. For example, when the model makes a small mistake, it will find itself in a state that is a little different, and as a result it will make a bigger mistake. And this mistake compounds in a sequential decision-making setting, eventually the learnt policy will do something very different from the demonstrated behaviour.

Another problem is that the loss function that we try to minimize, or the objective function that we try to maximize, is conditioned upon the distribution of the training data and not the actual policy! The distributional shift caused by off-policy training results in us not exactly learning the right objective.

Sometimes works well:
\begin{enumerate}
    \item Hacks (e.g. left/right images)\\
    Nvidia managed to use behavioural cloning to drive an autonomous vehicle in 2016 because of a slight modification, and that is having 3 cameras at the front - one pointing to the left, one pointing in the center, and one pointing to the right. The center camera is supervised by the steering angle of the driver, the left camera is supervised by the steering angle a little to the right of what the driver did, vice versa for the right camera. This mitigates the drifting problem because the left and right camera are essentially teaching the policy how to correct those small mistakes, preventing them from accumulating!
    \item Samples from a stable trajectory distribution
    \item Add more \textbf{on-policy} data, e.g. using Dagger
    \item Better models that fit more accurately
\end{enumerate}

Another idea for imitation learning: 
\begin{enumerate}
    \item Start with random policy
    \item Collect data with random goals
    \item Treat this trajectory as "demostrations" for the goals that were reached (aka the terminal states become our goals!)
    \item Use this to improve the policy - our policy is now conditioned not only on the action, but also our goal, $\pi_\theta(s_t | a_t, p)$, where $p$ is the random goal we reached
    \item repeat
\end{enumerate}
% \subsection{Distributional Shift}
% Revisiting the problem of drifting policy, the reason why this is observed is because when we run our policy, the distribution over observations that we see is different from the distribution of our training data, which is because the policy takes different actions! $p_{data}(o_t) != p_{\pi_\theta}(o_t)$

% It's really hard to make the policy perfect such that $p_{\pi_\theta}(o_t)$ has the same distribution as training data, while preventing our policy from overfitting on the human data (training data).

% Reasons why it is very hard for policy to perfectly follow the expert's demonstration:
% \begin{enumerate}
%     \item Non-Markovian behaviour
%     If we see the same thing twice, we should behave the exact same way. This is often very unnatural for human demonstrators, because the actions we take often depend on events that occurred in the past.
%     \item Multimodal behaviour
%     When we see an obstacle, we may sometime choose to go left, other times we may choose to go right to go around the obstacle.
% \end{enumerate}

\section{RL Intro}
The goal of reinforcement learning is to find the optimal parameter $\theta^*$, such that 
\begin{equation}\label{eq:rl_goal}
    \theta^* = argmax_\theta\, E_{\tau \sim p_\theta(\tau)}\,[\sum_t r(s_t, a_t)]
\end{equation}
where
\begin{itemize}
    \item $\tau$ is a sequences of states and actions, which is a set of trajectories $(s_1, a_1, ..., s_T, a_T)$
    \item $p_\theta(\tau) = p(s_1)\: \Pi^T_{t=1}\,\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)$, which represents the \textbf{trajectory distribution}, can intuitively be understood as the true probability of experiencing trajectory $\tau$ under policy with param $\theta$
\end{itemize}
Note that we are taking the expected sum of rewards over trajectory $\tau$ with respect to the trajectory distribution $p_\theta(\tau)$ under policy $\pi$ with parameter $\theta$. And we take the expectation because of our limited knowledge of the world - i.e. we are only able to maximize the expectation over trajectories we have seen. Intuitively, this also meant that the more trajectories we see, the more accurate our estimate will be - we will come back to this soon.

It should be noted that Equation \ref{eq:rl_goal} can also be written as $\theta^* = argmax_\theta \; \sum_{t=1}^T\, E_{(s_t, a_t) \sim p_\theta(s_t, a_t)}[r(s_t, a_t)]$ for finite horizon case, and $\theta^* = argmax_\theta \; E_{(s_t, a_t) \sim p_\theta(s_t, a_t)}[r(s_t, a_t)]$ for infinite horizon case.

\section{Policy Gradients}
\subsection{Evaluating the objective}
Recall the objective $\theta^* = argmax_\theta\, E_{\tau \sim p_\theta(\tau)}\,[\sum_t r(s_t, a_t)]$. Let $J(\theta) = E_{\tau \sim p_\theta(\tau)}\,[\sum_t r(s_t, a_t)]$ be our objective to maximize. Our goal is to find a set of param $\theta$ that maximizes our objective function, $J(\theta)$

To do so, we will need to be able to take the expected sum of rewards over trajectory $\tau$ with respect $p_\theta(\tau)$. We know that $p_\theta(\tau) = p(s_1)\: \Pi^T_{t=1}\,\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t)$, but since we do not know transition probabilities $p(s_{t+1}|s_t, a_t)$, nor do we know initial state distribution $p(s_1)$, we instead estimate $J(\theta)$ by averaging over $N$ rollouts of our policy, which samples from the initial state distribution and the transition probabilities. This gives us: 

\begin{equation}\label{eq:objective_funct}
    J(\theta) \approx \frac{1}{N}\sum_i\sum_t r(s_{i,t}, a_{i,t})
\end{equation}

Going back to the point of getting better estimation with more trajectories, the larger our $N$ is, the better we will be able to estimate the value of $J(\theta)$.

\subsection{Improving our policy}
Now that we know how to estimate $J(\theta)$ without relying on any underlying understanding of the model of the world, we can now try to improve param $\theta$ by taking gradient ascent on $J(\theta)$ (we want to maximize $J(\theta)$, that is, by computing the gradient of our objective as such: $\nabla_\theta J(\theta)$.

We first write $J(\theta)$ as an integral:
\begin{equation*}
\begin{split}
        J(\theta) &= E_{\tau \sim p_\theta(\tau)}\,[\sum_t r(s_t, a_t)] \\ &= E_{\tau \sim p_\theta(\tau)}\,[r(\tau)] \\
        &= \int p_\theta(\tau)r(\tau)d\tau
\end{split}
\end{equation*}

Taking the derivative,
\begin{equation}\label{eq:derivative_obj_funct}
\begin{split}
    \nabla_\theta J(\theta) &= \int \nabla_\theta\, p_\theta(\tau)r(\tau)d\tau \\
    &= \int p_\theta(\tau) \nabla_\theta\, log\, p_\theta (\tau) r(\tau) d\tau \\
    &= E_{\tau \sim p_\theta(\tau)}\,[\nabla_\theta\, log\, p_\theta(\tau)r(\tau)]
\end{split}
\end{equation}
Note that the identity $p_\theta(\tau) \nabla_\theta\, log\, p_\theta (\tau) = p_\theta(\tau) \frac{\nabla_\theta\, p_\theta(\tau)}{p_\theta(\tau)} = \nabla_\theta\, p_\theta(\tau)$ was used from line 1 to 2 in the equation above.

Before proceeding, we take logs on both sides of the equation 
\begin{equation}\label{eq:logptheta}
\begin{split}
    p_\theta(\tau) &= p(s_1)\: \Pi^T_{t=1}\,\pi_\theta(a_t|s_t)p(s_{t+1}|s_t, a_t) \\ log\, p_\theta(\tau) &= log\, p(s_1) + \sum_{t=1}^T\: log\,\pi_\theta(a_t|s_t) + log\, p(s_{t+1}|s_t, a_t) 
\end{split}
\end{equation}

Substituting equation \ref{eq:logptheta} into \ref{eq:derivative_obj_funct}, we get \begin{equation*}
\begin{split}
    J(\theta) &= E_{\tau \sim p_\theta(\tau)}\,[\nabla_\theta\, (\sum_{t=1}^T log\,\pi_\theta(a_t|s_t))\,r(\tau)] \\
    &\approx lala 
\end{split}
\end{equation*}
Note that when we are substituting equation \ref{eq:logptheta} into \ref{eq:derivative_obj_funct}, derivative with respect to $\theta$ for $log\, p(s_1)$ and $log\, p(s_{t+1}|s_t, a_t)$ equates to 0 since they do not rely on $\theta$ and thus those terms disappear. 
\newpage
\bibliographystyle{unsrt}
\bibliography{references}

\end{document}


