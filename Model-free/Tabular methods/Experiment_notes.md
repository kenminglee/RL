### Interesting Findings about environments:
Since the Frozen Lake Environment only returns a 1 for reaching the Goal state and 0 for falling into a pit, the lack of feedback meant that it takes very very long for the value of the goal state to slowly propagate through the entire path. If falling into the pit returns -1, or if every step returns -0.1, then the agent will be more incentivized to find the most efficient path. And adding slipperiness of the ice into the environment will make it only harder for the agent to learn the safe path.

The reward engineering of this environment makes it hard for our agent to learn effectively. This is why we used the Taxi-v3 environment for Sarsa instead. The Taxi-v3 environment is arguably more complex (larger number of total states * actions), and the randomness of the pickup/dropoff point means the agent will need to learn the randomness of the environment as well. However, due to the nature of the reward feedback that it gets, it allows our agent to learn a lot more effectively, and the result plots can show clear progress across 30000 episodes.

For Sarsa, adding epsilon-greedy decay over time helps us to reduce the variance as the agent converges. This is because whenever our agent takes a non-greedy action, it affects the value-estimation of that state since the agent now thinks that the state is less valuable since it has taken a less-good action. This causes us to have high variance. Therefore reducing the probability of taking non-greedy actions drastically reduces variance in the reward received per episode.

